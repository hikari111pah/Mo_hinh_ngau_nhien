{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SHFFH', 'FFFHF', 'FFFHF', 'HHFHH', 'FFFFG']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_random_map(size, p):\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    :param size: size of each side of the grid\n",
    "    :param p: probability that a tile is frozen\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "\n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(res):\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((0,0))\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r,c) in discovered:\n",
    "                discovered.add((r,c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
    "                        continue\n",
    "                    if res[r_new][c_new] == 'G':\n",
    "                        return True\n",
    "                    if (res[r_new][c_new] !='H'):\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    while not valid:\n",
    "        p = min(1, p)\n",
    "        res = np.random.choice(['F', 'H'], (size, size), p=[p, 1-p])\n",
    "        res[0][0] = 'S'\n",
    "        res[-1][-1] = 'G'\n",
    "        valid = is_valid(res)\n",
    "    return [\"\".join(x) for x in res]\n",
    "\n",
    "generate_random_map(5,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_map = ['SFFFF', 'FHFFF', 'FFFFH', 'HFHFH', 'FHFFG']\n",
    "gamma= 0.99\n",
    "eps=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFF\n",
      "FHFFF\n",
      "FFFFH\n",
      "HFHFH\n",
      "FHFFG\n",
      "Converged after 6 policy iterations\n",
      "Won 262 of 1000 games!\n",
      "[[0.15541242 0.16039381 0.17038269 0.17685614 0.17418432]\n",
      " [0.13330009 0.         0.16925812 0.18507288 0.1769174 ]\n",
      " [0.11537357 0.10104916 0.15754923 0.20715243 0.        ]\n",
      " [0.         0.03334622 0.         0.28512501 0.        ]\n",
      " [0.         0.         0.32352941 0.65686275 0.        ]]\n",
      "[[3. 3. 2. 2. 0.]\n",
      " [0. 0. 2. 2. 3.]\n",
      " [3. 1. 3. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def eval_state_action(V, s, a, gamma=gamma):\n",
    "    return np.sum([p * (rew + gamma*V[next_s]) for p, next_s, rew, _ in env.P[s][a]])\n",
    "\n",
    "def policy_evaluation(V, policy, eps=eps):\n",
    "    '''\n",
    "    Policy evaluation. Update the value function until it reach a steady state\n",
    "    '''\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # loop over all states\n",
    "        for s in range(nS):\n",
    "            old_v = V[s]\n",
    "            # update V[s] using the Bellman equation\n",
    "            V[s] = eval_state_action(V, s, policy[s])\n",
    "            delta = max(delta, np.abs(old_v - V[s]))\n",
    "\n",
    "        if delta < eps:\n",
    "            break\n",
    "\n",
    "def policy_improvement(V, policy):\n",
    "    '''\n",
    "    Policy improvement. Update the policy based on the value function\n",
    "    '''\n",
    "    policy_stable = True\n",
    "    for s in range(nS):\n",
    "        old_a = policy[s]\n",
    "        # update the policy with the action that bring to the highest state value\n",
    "        policy[s] = np.argmax([eval_state_action(V, s, a) for a in range(nA)])\n",
    "        if old_a != policy[s]: \n",
    "            policy_stable = False\n",
    "\n",
    "    return policy_stable\n",
    "\n",
    "\n",
    "def run_episodes(env, policy, num_games=1000):\n",
    "    '''\n",
    "    Run some games to test a policy\n",
    "    '''\n",
    "    tot_rew = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        done = False\n",
    "        while not done:\n",
    "            # select the action accordingly to the policy\n",
    "            next_state, reward, done, _ = env.step(policy[state])\n",
    "                \n",
    "            state = next_state\n",
    "            tot_rew += reward \n",
    "            if done:\n",
    "                state = env.reset()\n",
    "\n",
    "    print('Won %i of %i games!'%(tot_rew, num_games))\n",
    "\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    # create the environment\n",
    "    #desc= custom_map  or desc= generate_random_map(size,p) \n",
    "    env = gym.make('FrozenLake-v0',desc= custom_map)\n",
    "    # enwrap it to have additional information from it\n",
    "    env = env.unwrapped\n",
    "    env.render()\n",
    "\n",
    "    # spaces dimension\n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.n\n",
    "    \n",
    "    # initializing value function and policy\n",
    "    V = np.zeros(nS)\n",
    "    policy = np.zeros(nS)\n",
    "\n",
    "    # some useful variable\n",
    "    policy_stable = False\n",
    "    it = 0\n",
    "\n",
    "    while not policy_stable:\n",
    "        policy_evaluation(V, policy)\n",
    "        policy_stable = policy_improvement(V, policy)\n",
    "        it += 1\n",
    "\n",
    "    print('Converged after %i policy iterations'%(it))\n",
    "    run_episodes(env, policy)\n",
    "    print(V.reshape((s,s)))\n",
    "    print(policy.reshape((s,s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFF\n",
      "FHFFF\n",
      "FFFFH\n",
      "HFHFH\n",
      "FHFFG\n",
      "Iter: 0  delta: 0.33333\n",
      "Iter: 1  delta: 0.1463\n",
      "Iter: 2  delta: 0.08458\n",
      "Iter: 3  delta: 0.05305\n",
      "Iter: 4  delta: 0.03158\n",
      "Iter: 5  delta: 0.01851\n",
      "Iter: 6  delta: 0.01114\n",
      "Iter: 7  delta: 0.0099\n",
      "Iter: 8  delta: 0.00928\n",
      "Iter: 9  delta: 0.00901\n",
      "Iter: 10  delta: 0.00869\n",
      "Iter: 11  delta: 0.0082\n",
      "Iter: 12  delta: 0.00765\n",
      "Iter: 13  delta: 0.00708\n",
      "Iter: 14  delta: 0.00652\n",
      "Iter: 15  delta: 0.006\n",
      "Iter: 16  delta: 0.0058\n",
      "Iter: 17  delta: 0.00561\n",
      "Iter: 18  delta: 0.00555\n",
      "Iter: 19  delta: 0.00543\n",
      "Iter: 20  delta: 0.00526\n",
      "Iter: 21  delta: 0.00507\n",
      "Iter: 22  delta: 0.00485\n",
      "Iter: 23  delta: 0.00462\n",
      "Iter: 24  delta: 0.00439\n",
      "Iter: 25  delta: 0.00416\n",
      "Iter: 26  delta: 0.00393\n",
      "Iter: 27  delta: 0.00371\n",
      "Iter: 28  delta: 0.0035\n",
      "Iter: 29  delta: 0.00331\n",
      "Iter: 30  delta: 0.00313\n",
      "Iter: 31  delta: 0.00296\n",
      "Iter: 32  delta: 0.00281\n",
      "Iter: 33  delta: 0.00266\n",
      "Iter: 34  delta: 0.00252\n",
      "Iter: 35  delta: 0.00238\n",
      "Iter: 36  delta: 0.00226\n",
      "Iter: 37  delta: 0.00214\n",
      "Iter: 38  delta: 0.00203\n",
      "Iter: 39  delta: 0.00192\n",
      "Iter: 40  delta: 0.00182\n",
      "Iter: 41  delta: 0.00173\n",
      "Iter: 42  delta: 0.00164\n",
      "Iter: 43  delta: 0.00155\n",
      "Iter: 44  delta: 0.00147\n",
      "Iter: 45  delta: 0.0014\n",
      "Iter: 46  delta: 0.00132\n",
      "Iter: 47  delta: 0.00126\n",
      "Iter: 48  delta: 0.00119\n",
      "Iter: 49  delta: 0.00113\n",
      "Iter: 50  delta: 0.00107\n",
      "Iter: 51  delta: 0.00102\n",
      "Iter: 52  delta: 0.00096\n",
      "Iter: 53  delta: 0.00092\n",
      "Iter: 54  delta: 0.00087\n",
      "Iter: 55  delta: 0.00082\n",
      "Iter: 56  delta: 0.00078\n",
      "Iter: 57  delta: 0.00074\n",
      "Iter: 58  delta: 0.0007\n",
      "Iter: 59  delta: 0.00067\n",
      "Iter: 60  delta: 0.00063\n",
      "Iter: 61  delta: 0.0006\n",
      "Iter: 62  delta: 0.00057\n",
      "Iter: 63  delta: 0.00054\n",
      "Iter: 64  delta: 0.00051\n",
      "Iter: 65  delta: 0.00049\n",
      "Iter: 66  delta: 0.00046\n",
      "Iter: 67  delta: 0.00044\n",
      "Iter: 68  delta: 0.00042\n",
      "Iter: 69  delta: 0.00039\n",
      "Iter: 70  delta: 0.00037\n",
      "Iter: 71  delta: 0.00035\n",
      "Iter: 72  delta: 0.00034\n",
      "Iter: 73  delta: 0.00032\n",
      "Iter: 74  delta: 0.0003\n",
      "Iter: 75  delta: 0.00029\n",
      "Iter: 76  delta: 0.00027\n",
      "Iter: 77  delta: 0.00026\n",
      "Iter: 78  delta: 0.00025\n",
      "Iter: 79  delta: 0.00023\n",
      "Iter: 80  delta: 0.00022\n",
      "Iter: 81  delta: 0.00021\n",
      "Iter: 82  delta: 0.0002\n",
      "Iter: 83  delta: 0.00019\n",
      "Iter: 84  delta: 0.00018\n",
      "Iter: 85  delta: 0.00017\n",
      "Iter: 86  delta: 0.00016\n",
      "Iter: 87  delta: 0.00015\n",
      "Iter: 88  delta: 0.00014\n",
      "Iter: 89  delta: 0.00014\n",
      "Iter: 90  delta: 0.00013\n",
      "Iter: 91  delta: 0.00012\n",
      "Iter: 92  delta: 0.00012\n",
      "Iter: 93  delta: 0.00011\n",
      "Iter: 94  delta: 0.00011\n",
      "Iter: 95  delta: 0.0001\n",
      "Won 272 of 1000 games!\n",
      "[[0.1553873  0.16036959 0.17036065 0.17683372 0.17416158]\n",
      " [0.13328005 0.         0.16923929 0.18505378 0.1768962 ]\n",
      " [0.11535803 0.10103788 0.15753499 0.20714001 0.        ]\n",
      " [0.         0.0333425  0.         0.28512091 0.        ]\n",
      " [0.         0.         0.32352941 0.65686275 0.        ]]\n",
      "[[3. 3. 2. 2. 0.]\n",
      " [0. 0. 2. 2. 3.]\n",
      " [3. 1. 3. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def eval_state_action(V, s, a, gamma=gamma):\n",
    "    return np.sum([p * (rew + gamma*V[next_s]) for p, next_s, rew, _ in env.P[s][a]])\n",
    "\n",
    "def value_iteration(eps=eps):\n",
    "    '''\n",
    "    Value iteration algorithm\n",
    "    '''\n",
    "    V = np.zeros(nS)\n",
    "    it = 0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # update the value of each state using as \"policy\" the max operator\n",
    "        for s in range(nS):\n",
    "            old_v = V[s]\n",
    "            V[s] = np.max([eval_state_action(V, s, a) for a in range(nA)])\n",
    "            delta = max(delta, np.abs(old_v - V[s]))\n",
    "\n",
    "        if delta < eps:\n",
    "            break\n",
    "        else:\n",
    "            print('Iter:', it, ' delta:', np.round(delta, 5))\n",
    "        it += 1\n",
    "\n",
    "    return V\n",
    "\n",
    "def run_episodes(env, V, num_games=1000):\n",
    "    '''\n",
    "    Run some test games\n",
    "    '''\n",
    "    tot_rew = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.argmax([eval_state_action(V, state, a) for a in range(nA)])\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            tot_rew += reward \n",
    "            if done:\n",
    "                state = env.reset()\n",
    "\n",
    "    print('Won %i of %i games!'%(tot_rew, num_games))\n",
    "\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    # create the environment\n",
    "    #desc= custom_map  or desc= generate_random_map(size,p) \n",
    "    s=5\n",
    "    env = gym.make('FrozenLake-v0',desc= custom_map)\n",
    "    # enwrap it to have additional information from it\n",
    "    env = env.unwrapped\n",
    "    env.render()\n",
    "\n",
    "    # spaces dimension\n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.n\n",
    "\n",
    "    # Value iteration\n",
    "    V = value_iteration(eps=0.0001)\n",
    "    # test the value function on 100 games\n",
    "    run_episodes(env, V, 1000)\n",
    "    # print the state values\n",
    "    print(V.reshape((5,5)))\n",
    "    print(policy.reshape((5,5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
