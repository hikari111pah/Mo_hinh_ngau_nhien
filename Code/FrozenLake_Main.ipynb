{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SFFFF', 'FFFFF', 'FFFHF', 'FFFFF', 'FHHFG']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def generate_random_map(size, p):\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    :param size: size of each side of the grid\n",
    "    :param p: probability that a tile is frozen\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "\n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(res):\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((0,0))\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r,c) in discovered:\n",
    "                discovered.add((r,c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
    "                        continue\n",
    "                    if res[r_new][c_new] == 'G':\n",
    "                        return True\n",
    "                    if (res[r_new][c_new] !='H'):\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    while not valid:\n",
    "        p = min(1, p)\n",
    "        res = np.random.choice(['F', 'H'], (size, size), p=[p, 1-p])\n",
    "        res[0][0] = 'S'\n",
    "        res[-1][-1] = 'G'\n",
    "        valid = is_valid(res)\n",
    "    return [\"\".join(x) for x in res]\n",
    "\n",
    "generate_random_map(5,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_map = ['SFFFH', 'FHFFF', 'FFHFH', 'FFFFF', 'FHFFG']\n",
    "gamma= 0.99\n",
    "eps=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFH\n",
      "FHFFF\n",
      "FFHFH\n",
      "FFFFF\n",
      "FHFFG\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 2. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 2. 1.]\n",
      " [0. 0. 1. 2. 0.]]\n",
      "[[0. 1. 2. 0. 0.]\n",
      " [0. 0. 2. 2. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1.]\n",
      " [0. 0. 2. 2. 0.]]\n",
      "[[1. 2. 2. 0. 0.]\n",
      " [0. 0. 2. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 2. 1. 1. 1.]\n",
      " [0. 0. 2. 2. 0.]]\n",
      "[[0. 3. 2. 0. 0.]\n",
      " [0. 0. 2. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 3. 1. 1. 1.]\n",
      " [0. 0. 2. 2. 0.]]\n",
      "[[0. 3. 3. 0. 0.]\n",
      " [0. 0. 2. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 3. 1. 1. 1.]\n",
      " [0. 0. 2. 2. 0.]]\n",
      "Converged after 7 policy iterations\n",
      "Won 579 of 1000 games!\n",
      "[[0.36622453 0.32930569 0.30253772 0.28507986 0.        ]\n",
      " [0.37759253 0.         0.19102399 0.27636208 0.13610881]\n",
      " [0.40054318 0.28460439 0.         0.36137257 0.        ]\n",
      " [0.43572637 0.46192058 0.67944319 0.8187138  0.90075766]\n",
      " [0.42283587 0.         0.77830134 0.90075766 0.        ]]\n",
      "[[0. 3. 3. 0. 0.]\n",
      " [0. 0. 2. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 3. 1. 1. 1.]\n",
      " [0. 0. 2. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Thuật toán lặp chính sách\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def eval_state_action(V, s, a, gamma=gamma):\n",
    "    return np.sum([p * (rew + gamma*V[next_s]) for p, next_s, rew, _ in env.P[s][a]])\n",
    "\n",
    "def policy_evaluation(V, policy, eps=eps):\n",
    "    '''\n",
    "    Policy evaluation. Update the value function until it reach a steady state\n",
    "    '''\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # loop over all states\n",
    "        for s in range(nS):\n",
    "            old_v = V[s]\n",
    "            # update V[s] using the Bellman equation\n",
    "            V[s] = eval_state_action(V, s, policy[s])\n",
    "            delta = max(delta, np.abs(old_v - V[s]))\n",
    "\n",
    "        if delta < eps:\n",
    "            break\n",
    "\n",
    "def policy_improvement(V, policy):\n",
    "    '''\n",
    "    Policy improvement. Update the policy based on the value function\n",
    "    '''\n",
    "    policy_stable = True\n",
    "    for s in range(nS):\n",
    "        old_a = policy[s]\n",
    "        # update the policy with the action that bring to the highest state value\n",
    "        policy[s] = np.argmax([eval_state_action(V, s, a) for a in range(nA)])\n",
    "        if old_a != policy[s]: \n",
    "            policy_stable = False\n",
    "\n",
    "    return policy_stable\n",
    "\n",
    "\n",
    "def run_episodes(env, policy, num_games=1000):\n",
    "    '''\n",
    "    Run some games to test a policy\n",
    "    '''\n",
    "    tot_rew = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        done = False\n",
    "        while not done:\n",
    "            # select the action accordingly to the policy\n",
    "            next_state, reward, done, _ = env.step(policy[state])\n",
    "                \n",
    "            state = next_state\n",
    "            tot_rew += reward \n",
    "            if done:\n",
    "                state = env.reset()\n",
    "\n",
    "    print('Won %i of %i games!'%(tot_rew, num_games))\n",
    "\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    # create the environment\n",
    "    #desc= custom_map  or desc= generate_random_map(size,p) \n",
    "    env = gym.make('FrozenLake-v0',desc= custom_map)\n",
    "    # enwrap it to have additional information from it\n",
    "    env = env.unwrapped\n",
    "    env.render()\n",
    "\n",
    "    # spaces dimension\n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.n\n",
    "    \n",
    "    # initializing value function and policy\n",
    "    V = np.zeros(nS)\n",
    "    policy = np.zeros(nS)\n",
    "\n",
    "    # some useful variable\n",
    "    policy_stable = False\n",
    "    it = 0\n",
    "\n",
    "    while not policy_stable:\n",
    "        print(policy.reshape((5,5)))\n",
    "        policy_evaluation(V, policy)\n",
    "        policy_stable = policy_improvement(V, policy)\n",
    "        it += 1\n",
    "\n",
    "    print('Converged after %i policy iterations'%(it))\n",
    "    run_episodes(env, policy)\n",
    "    print(V.reshape((5,5)))\n",
    "    print(policy.reshape((5,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFH\n",
      "FHFFF\n",
      "FFHFH\n",
      "FFFFF\n",
      "FHFFG\n",
      "Iter: 0  delta: 0.33333\n",
      "Iter: 1  delta: 0.22\n",
      "Iter: 2  delta: 0.15645\n",
      "Iter: 3  delta: 0.11603\n",
      "Iter: 4  delta: 0.0916\n",
      "Iter: 5  delta: 0.06954\n",
      "Iter: 6  delta: 0.05371\n",
      "Iter: 7  delta: 0.04149\n",
      "Iter: 8  delta: 0.03227\n",
      "Iter: 9  delta: 0.0256\n",
      "Iter: 10  delta: 0.02357\n",
      "Iter: 11  delta: 0.02194\n",
      "Iter: 12  delta: 0.02073\n",
      "Iter: 13  delta: 0.01919\n",
      "Iter: 14  delta: 0.01748\n",
      "Iter: 15  delta: 0.0162\n",
      "Iter: 16  delta: 0.0151\n",
      "Iter: 17  delta: 0.01407\n",
      "Iter: 18  delta: 0.01308\n",
      "Iter: 19  delta: 0.01214\n",
      "Iter: 20  delta: 0.01124\n",
      "Iter: 21  delta: 0.01069\n",
      "Iter: 22  delta: 0.01023\n",
      "Iter: 23  delta: 0.00974\n",
      "Iter: 24  delta: 0.00924\n",
      "Iter: 25  delta: 0.00873\n",
      "Iter: 26  delta: 0.00823\n",
      "Iter: 27  delta: 0.00788\n",
      "Iter: 28  delta: 0.00761\n",
      "Iter: 29  delta: 0.00737\n",
      "Iter: 30  delta: 0.00714\n",
      "Iter: 31  delta: 0.0069\n",
      "Iter: 32  delta: 0.00666\n",
      "Iter: 33  delta: 0.00642\n",
      "Iter: 34  delta: 0.00618\n",
      "Iter: 35  delta: 0.00593\n",
      "Iter: 36  delta: 0.00568\n",
      "Iter: 37  delta: 0.00544\n",
      "Iter: 38  delta: 0.0052\n",
      "Iter: 39  delta: 0.00496\n",
      "Iter: 40  delta: 0.00473\n",
      "Iter: 41  delta: 0.00451\n",
      "Iter: 42  delta: 0.00429\n",
      "Iter: 43  delta: 0.00408\n",
      "Iter: 44  delta: 0.00387\n",
      "Iter: 45  delta: 0.00368\n",
      "Iter: 46  delta: 0.00349\n",
      "Iter: 47  delta: 0.00331\n",
      "Iter: 48  delta: 0.00313\n",
      "Iter: 49  delta: 0.00297\n",
      "Iter: 50  delta: 0.00281\n",
      "Iter: 51  delta: 0.00266\n",
      "Iter: 52  delta: 0.00252\n",
      "Iter: 53  delta: 0.00238\n",
      "Iter: 54  delta: 0.00225\n",
      "Iter: 55  delta: 0.00212\n",
      "Iter: 56  delta: 0.00201\n",
      "Iter: 57  delta: 0.0019\n",
      "Iter: 58  delta: 0.00179\n",
      "Iter: 59  delta: 0.00169\n",
      "Iter: 60  delta: 0.00159\n",
      "Iter: 61  delta: 0.0015\n",
      "Iter: 62  delta: 0.00142\n",
      "Iter: 63  delta: 0.00134\n",
      "Iter: 64  delta: 0.00126\n",
      "Iter: 65  delta: 0.00119\n",
      "Iter: 66  delta: 0.00112\n",
      "Iter: 67  delta: 0.00106\n",
      "Iter: 68  delta: 0.001\n",
      "Iter: 69  delta: 0.00094\n",
      "Iter: 70  delta: 0.00089\n",
      "Iter: 71  delta: 0.00083\n",
      "Iter: 72  delta: 0.00079\n",
      "Iter: 73  delta: 0.00074\n",
      "Iter: 74  delta: 0.0007\n",
      "Iter: 75  delta: 0.00066\n",
      "Iter: 76  delta: 0.00062\n",
      "Iter: 77  delta: 0.00058\n",
      "Iter: 78  delta: 0.00055\n",
      "Iter: 79  delta: 0.00052\n",
      "Iter: 80  delta: 0.00049\n",
      "Iter: 81  delta: 0.00046\n",
      "Iter: 82  delta: 0.00043\n",
      "Iter: 83  delta: 0.00041\n",
      "Iter: 84  delta: 0.00038\n",
      "Iter: 85  delta: 0.00036\n",
      "Iter: 86  delta: 0.00034\n",
      "Iter: 87  delta: 0.00032\n",
      "Iter: 88  delta: 0.0003\n",
      "Iter: 89  delta: 0.00028\n",
      "Iter: 90  delta: 0.00027\n",
      "Iter: 91  delta: 0.00025\n",
      "Iter: 92  delta: 0.00024\n",
      "Iter: 93  delta: 0.00022\n",
      "Iter: 94  delta: 0.00021\n",
      "Iter: 95  delta: 0.0002\n",
      "Iter: 96  delta: 0.00019\n",
      "Iter: 97  delta: 0.00017\n",
      "Iter: 98  delta: 0.00016\n",
      "Iter: 99  delta: 0.00015\n",
      "Iter: 100  delta: 0.00015\n",
      "Iter: 101  delta: 0.00014\n",
      "Iter: 102  delta: 0.00013\n",
      "Iter: 103  delta: 0.00012\n",
      "Iter: 104  delta: 0.00011\n",
      "Iter: 105  delta: 0.00011\n",
      "Iter: 106  delta: 0.0001\n",
      "Won 579 of 1000 games!\n",
      "[[0.36621021 0.32929648 0.30253222 0.2850765  0.        ]\n",
      " [0.37757871 0.         0.19102151 0.27635968 0.13610773]\n",
      " [0.40053051 0.28459772 0.         0.36137105 0.        ]\n",
      " [0.43571592 0.46191353 0.67943923 0.81871175 0.90075661]\n",
      " [0.42282392 0.         0.77829875 0.90075661 0.        ]]\n",
      "[[0. 3. 3. 0. 0.]\n",
      " [0. 0. 2. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 3. 1. 1. 1.]\n",
      " [0. 0. 2. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Thuật toán lặp giá trị\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def eval_state_action(V, s, a, gamma=gamma):\n",
    "    return np.sum([p * (rew + gamma*V[next_s]) for p, next_s, rew, _ in env.P[s][a]])\n",
    "\n",
    "def value_iteration(eps=eps):\n",
    "    '''\n",
    "    Value iteration algorithm\n",
    "    '''\n",
    "    V = np.zeros(nS)\n",
    "    it = 0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # update the value of each state using as \"policy\" the max operator\n",
    "        for s in range(nS):\n",
    "            old_v = V[s]\n",
    "            V[s] = np.max([eval_state_action(V, s, a) for a in range(nA)])\n",
    "            delta = max(delta, np.abs(old_v - V[s]))\n",
    "\n",
    "        if delta < eps:\n",
    "            break\n",
    "        else:\n",
    "            print('Iter:', it, ' delta:', np.round(delta, 5))\n",
    "        it += 1\n",
    "\n",
    "    return V\n",
    "\n",
    "def run_episodes(env, V, num_games=1000):\n",
    "    '''\n",
    "    Run some test games\n",
    "    '''\n",
    "    tot_rew = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.argmax([eval_state_action(V, state, a) for a in range(nA)])\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            tot_rew += reward \n",
    "            if done:\n",
    "                state = env.reset()\n",
    "\n",
    "    print('Won %i of %i games!'%(tot_rew, num_games))\n",
    "\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    # create the environment\n",
    "    #desc= custom_map  or desc= generate_random_map(size,p) \n",
    "    s=5\n",
    "    env = gym.make('FrozenLake-v0',desc= custom_map)\n",
    "    # enwrap it to have additional information from it\n",
    "    env = env.unwrapped\n",
    "    env.render()\n",
    "\n",
    "    # spaces dimension\n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.n\n",
    "\n",
    "    # Value iteration\n",
    "    V = value_iteration(eps=0.0001)\n",
    "    # test the value function on 100 games\n",
    "    run_episodes(env, V)\n",
    "    # print the state values\n",
    "    print(V.reshape((5,5)))\n",
    "    print(policy.reshape((5,5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
